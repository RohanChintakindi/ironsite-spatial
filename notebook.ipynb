{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ironsite Spatial Awareness Pipeline\n",
    "\n",
    "**Video → Undistort → Grounded SAM 2 → VGGT-X → Scene Graphs → FAISS Memory → VLM**\n",
    "\n",
    "Processes body cam video from construction workers and produces structured spatial data for LLM-based activity analysis.\n",
    "\n",
    "---\n",
    "**Setup:** Run Cell 1 first (one-time install), then run cells sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Setup & Installation (run once per instance)\n",
    "# ============================================================\n",
    "import subprocess, os, sys\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers supervision opencv-python-headless numpy plotly matplotlib\n",
    "!pip install -q huggingface_hub openai scipy Pillow\n",
    "!pip install -q pycolmap\n",
    "!pip install -q faiss-gpu 2>/dev/null || pip install -q faiss-cpu\n",
    "\n",
    "# Clone & install Grounded SAM 2\n",
    "if not os.path.exists(\"Grounded-SAM-2\"):\n",
    "    !git clone https://github.com/IDEA-Research/Grounded-SAM-2.git\n",
    "    %cd Grounded-SAM-2\n",
    "    !SAM2_BUILD_CUDA=0 pip install -e \".[notebooks]\" -q\n",
    "    %cd checkpoints\n",
    "    !bash download_ckpts.sh\n",
    "    %cd ../..\n",
    "else:\n",
    "    print(\"Grounded-SAM-2 already installed\")\n",
    "\n",
    "# Clone & install VGGT-X\n",
    "if not os.path.exists(\"VGGT-X\"):\n",
    "    !git clone --recursive https://github.com/Linketic/VGGT-X.git\n",
    "    !pip install -q -r VGGT-X/requirements.txt\n",
    "else:\n",
    "    print(\"VGGT-X already installed\")\n",
    "\n",
    "# GPU check\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    name = torch.cuda.get_device_name(0)\n",
    "    vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {name} | VRAM: {vram:.1f} GB | BF16: {torch.cuda.is_bf16_supported()}\")\n",
    "else:\n",
    "    print(\"WARNING: No GPU detected!\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Configuration\n",
    "# ============================================================\n",
    "# Edit these settings before running the pipeline\n",
    "\n",
    "# --- INPUT ---\n",
    "VIDEO_PATH = \"videos/IronsiteHackathonData/11_prep_masonry.mp4\"  # <-- change this\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "# --- VIDEO PREPROCESSING ---\n",
    "KEYFRAME_INTERVAL = 10   # extract every Nth frame (10 = ~1.5s at 15fps)\n",
    "MAX_FRAMES = 0           # 0 = no limit\n",
    "FISHEYE_K_SCALE = 0.8\n",
    "FISHEYE_D = [-0.3, 0.1, 0.0, 0.0]\n",
    "FISHEYE_BALANCE = 0.5\n",
    "\n",
    "# --- GROUNDED SAM 2 ---\n",
    "TEXT_PROMPT = (\n",
    "    \"person . concrete block . cinder block . rebar . trowel . bucket . \"\n",
    "    \"hard hat . safety vest . gloved hand . scaffolding . crane . \"\n",
    "    \"mortar . pipe . wall . ladder . wheelbarrow\"\n",
    ")\n",
    "DETECTION_THRESHOLD = 0.3\n",
    "REDETECT_EVERY = 50\n",
    "TRACK_CHUNK_SIZE = 100\n",
    "SAM2_CHECKPOINT = \"Grounded-SAM-2/checkpoints/sam2.1_hiera_small.pt\"\n",
    "SAM2_CONFIG = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "\n",
    "# --- VGGT-X ---\n",
    "VGGTX_DIR = \"VGGT-X\"\n",
    "VGGTX_CHUNK_SIZE = 512    # reduce to 128-256 if OOM\n",
    "VGGTX_MAX_QUERY_PTS = 2048\n",
    "VGGTX_SHARED_CAMERA = True\n",
    "VGGTX_USE_GA = True       # global alignment\n",
    "VGGTX_SAVE_DEPTH = True\n",
    "\n",
    "# --- VLM ---\n",
    "GROK_API_KEY = \"\"         # <-- paste your xAI/Grok key here (or leave empty to skip)\n",
    "GROK_MODEL = \"grok-3-fast\"\n",
    "GROK_BASE_URL = \"https://api.x.ai/v1\"\n",
    "\n",
    "print(f\"Video: {VIDEO_PATH}\")\n",
    "print(f\"Exists: {os.path.exists(VIDEO_PATH)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Video Preprocessing — Fisheye Undistortion + Keyframes\n",
    "# ============================================================\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.preprocess import extract_keyframes\n",
    "\n",
    "# Scene directory for VGGT-X (expects images/ subdirectory)\n",
    "scene_dir = os.path.join(OUTPUT_DIR, \"scene\")\n",
    "frames_dir = os.path.join(scene_dir, \"images\")\n",
    "\n",
    "t0 = time.time()\n",
    "keyframes, timestamps, frame_indices, fps, w, h = extract_keyframes(\n",
    "    VIDEO_PATH, frames_dir,\n",
    "    interval=KEYFRAME_INTERVAL,\n",
    "    k_scale=FISHEYE_K_SCALE,\n",
    "    D=FISHEYE_D,\n",
    "    balance=FISHEYE_BALANCE,\n",
    "    max_frames=MAX_FRAMES,\n",
    ")\n",
    "print(f\"Completed in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Show sample keyframes\n",
    "n_show = min(6, len(keyframes))\n",
    "fig, axes = plt.subplots(1, n_show, figsize=(4 * n_show, 4))\n",
    "sample_idx = np.linspace(0, len(keyframes) - 1, n_show, dtype=int)\n",
    "for ax, si in zip(axes, sample_idx):\n",
    "    ax.imshow(keyframes[si])\n",
    "    ax.set_title(f\"Frame {frame_indices[si]} | t={timestamps[si]:.1f}s\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(f\"Undistorted Keyframes ({len(keyframes)} total)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Grounded SAM 2 — Detection + Segmentation + Tracking\n",
    "# ============================================================\n",
    "import torch\n",
    "from utils.detection import run_grounded_sam2\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "t0 = time.time()\n",
    "all_detections, object_labels = run_grounded_sam2(\n",
    "    keyframes, frames_dir, device,\n",
    "    text_prompt=TEXT_PROMPT,\n",
    "    threshold=DETECTION_THRESHOLD,\n",
    "    redetect_every=REDETECT_EVERY,\n",
    "    sam2_checkpoint=SAM2_CHECKPOINT,\n",
    "    sam2_config=SAM2_CONFIG,\n",
    ")\n",
    "print(f\"\\nCompleted in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Show detections on sample frames\n",
    "n_show = min(5, len(keyframes))\n",
    "fig, axes = plt.subplots(1, n_show, figsize=(5 * n_show, 5))\n",
    "sample_idx = np.linspace(0, len(keyframes) - 1, n_show, dtype=int)\n",
    "\n",
    "for ax, si in zip(axes, sample_idx):\n",
    "    ax.imshow(keyframes[si])\n",
    "    for det in all_detections[si]:\n",
    "        x1, y1, x2, y2 = det[\"bbox\"]\n",
    "        rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1,\n",
    "                             linewidth=2, edgecolor='lime', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(x1, y1 - 3, det[\"label\"], color='yellow', fontsize=7,\n",
    "                weight='bold', bbox=dict(boxstyle='round,pad=0.1',\n",
    "                facecolor='black', alpha=0.7))\n",
    "    ax.set_title(f\"t={timestamps[si]:.1f}s | {len(all_detections[si])} dets\")\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Grounded SAM 2 Detections\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show segmentation masks for first sample\n",
    "si = sample_idx[0]\n",
    "if all_detections[si]:\n",
    "    fig, axes = plt.subplots(1, min(4, len(all_detections[si])), figsize=(16, 4))\n",
    "    if not hasattr(axes, '__len__'):\n",
    "        axes = [axes]\n",
    "    for ax, det in zip(axes, all_detections[si][:4]):\n",
    "        if det.get(\"mask\") is not None:\n",
    "            ax.imshow(det[\"mask\"], cmap='viridis')\n",
    "            ax.set_title(f\"{det['label']} (id={det['id']})\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.suptitle(f\"Segmentation Masks (frame {si})\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: VGGT-X — 3D Reconstruction + Depth + Trajectory\n",
    "# ============================================================\n",
    "from utils.depth import run_full_3d_pipeline\n",
    "\n",
    "t0 = time.time()\n",
    "recon_data = run_full_3d_pipeline(\n",
    "    scene_dir=scene_dir,\n",
    "    vggtx_dir=VGGTX_DIR,\n",
    "    chunk_size=VGGTX_CHUNK_SIZE,\n",
    "    max_query_pts=VGGTX_MAX_QUERY_PTS,\n",
    "    shared_camera=VGGTX_SHARED_CAMERA,\n",
    "    use_ga=VGGTX_USE_GA,\n",
    "    save_depth=VGGTX_SAVE_DEPTH,\n",
    "    num_keyframes=len(keyframes),\n",
    ")\n",
    "print(f\"\\nCompleted in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Show depth maps\n",
    "n_show = min(5, len(keyframes))\n",
    "fig, axes = plt.subplots(2, n_show, figsize=(5 * n_show, 8))\n",
    "sample_idx = np.linspace(0, len(keyframes) - 1, n_show, dtype=int)\n",
    "\n",
    "for col, si in enumerate(sample_idx):\n",
    "    axes[0, col].imshow(keyframes[si])\n",
    "    axes[0, col].set_title(f\"t={timestamps[si]:.1f}s\")\n",
    "    axes[0, col].axis(\"off\")\n",
    "\n",
    "    fname = f\"{si:06d}.jpg\"\n",
    "    depth = recon_data[\"depth_map_cache\"].get(fname)\n",
    "    if depth is not None:\n",
    "        im = axes[1, col].imshow(depth, cmap='turbo')\n",
    "        axes[1, col].set_title(f\"Depth: {depth.min():.1f}-{depth.max():.1f}m\")\n",
    "        plt.colorbar(im, ax=axes[1, col], fraction=0.046)\n",
    "    else:\n",
    "        axes[1, col].text(0.5, 0.5, \"No depth\", ha='center', va='center',\n",
    "                          transform=axes[1, col].transAxes)\n",
    "    axes[1, col].axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Keyframes + VGGT-X Depth Maps\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: 3D Point Cloud + Worker Trajectory (Interactive)\n",
    "# ============================================================\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "pts = recon_data[\"points_xyz\"]\n",
    "cam = recon_data[\"cam_positions_smooth\"]\n",
    "\n",
    "# Subsample for performance\n",
    "if len(pts) > 30000:\n",
    "    idx = np.random.choice(len(pts), 30000, replace=False)\n",
    "    viz_pts = pts[idx]\n",
    "    viz_rgb = recon_data[\"points_rgb\"][idx]\n",
    "else:\n",
    "    viz_pts = pts\n",
    "    viz_rgb = recon_data[\"points_rgb\"]\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Point cloud\n",
    "if len(viz_pts) > 0:\n",
    "    colors = [f'rgb({r},{g},{b})' for r, g, b in viz_rgb]\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=viz_pts[:, 0], y=viz_pts[:, 1], z=viz_pts[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=1, color=colors, opacity=0.5),\n",
    "        name='Point Cloud'\n",
    "    ))\n",
    "\n",
    "# Worker trajectory\n",
    "if len(cam) > 0:\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=cam[:, 0], y=cam[:, 1], z=cam[:, 2],\n",
    "        mode='lines+markers',\n",
    "        marker=dict(size=3, color='red'),\n",
    "        line=dict(color='red', width=4),\n",
    "        name='Worker Path'\n",
    "    ))\n",
    "    # Start/end markers\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[cam[0, 0]], y=[cam[0, 1]], z=[cam[0, 2]],\n",
    "        mode='markers', marker=dict(size=8, color='green', symbol='diamond'),\n",
    "        name='Start'\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[cam[-1, 0]], y=[cam[-1, 1]], z=[cam[-1, 2]],\n",
    "        mode='markers', marker=dict(size=8, color='blue', symbol='diamond'),\n",
    "        name='End'\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f\"3D Workspace ({len(pts)} points) + Worker Trajectory ({recon_data['total_distance']:.1f}m)\",\n",
    "    width=900, height=700,\n",
    "    scene=dict(aspectmode='data')\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Scene Graph Builder\n",
    "# ============================================================\n",
    "from utils.scene_graph import build_scene_graphs\n",
    "\n",
    "t0 = time.time()\n",
    "scene_graphs = build_scene_graphs(\n",
    "    keyframes, all_detections, recon_data, timestamps, frame_indices\n",
    ")\n",
    "print(f\"Completed in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# Show a sample scene graph\n",
    "import json\n",
    "sample_sg = scene_graphs[len(scene_graphs) // 2]  # middle frame\n",
    "print(f\"\\n--- Sample Scene Graph (frame {sample_sg['original_frame']}, t={sample_sg['timestamp_str']}) ---\")\n",
    "print(json.dumps(sample_sg, indent=2, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: FAISS Spatial Memory\n",
    "# ============================================================\n",
    "from utils.memory import SpatialMemory\n",
    "\n",
    "memory_dir = os.path.join(OUTPUT_DIR, \"memory_store\")\n",
    "memory = SpatialMemory(memory_dir)\n",
    "memory.ingest(scene_graphs, VIDEO_PATH)\n",
    "memory.save()\n",
    "\n",
    "print(f\"\\nMemory stats: {memory.stats()}\")\n",
    "\n",
    "# --- Demo queries ---\n",
    "print(\"\\n--- Spatial Queries ---\")\n",
    "\n",
    "blocks = memory.query_label(\"block\")\n",
    "print(f\"Frames with 'block': {len(blocks)}\")\n",
    "\n",
    "rebar = memory.query_label(\"rebar\")\n",
    "print(f\"Frames with 'rebar': {len(rebar)}\")\n",
    "\n",
    "close = memory.query_depth_range(0.5, 3.0)\n",
    "print(f\"Objects in work range (0.5-3m): {len(close)} frames\")\n",
    "\n",
    "person_near_block = memory.query_proximity(\"person\", \"block\", max_m=2.0)\n",
    "print(f\"Person near block (<2m): {len(person_near_block)} frames\")\n",
    "\n",
    "hand_near_block = memory.query_proximity(\"hand\", \"block\", max_m=1.0)\n",
    "print(f\"Hand near block (<1m): {len(hand_near_block)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: VLM Reasoning via Grok (optional)\n",
    "# ============================================================\n",
    "analysis_json = {}\n",
    "\n",
    "if GROK_API_KEY:\n",
    "    from utils.vlm import run_vlm_analysis\n",
    "\n",
    "    t0 = time.time()\n",
    "    analysis_json = run_vlm_analysis(\n",
    "        scene_graphs, VIDEO_PATH, GROK_API_KEY,\n",
    "        model=GROK_MODEL, base_url=GROK_BASE_URL,\n",
    "        num_samples=30, temperature=0.3, max_tokens=4000,\n",
    "    )\n",
    "    print(f\"\\nCompleted in {time.time() - t0:.1f}s\")\n",
    "else:\n",
    "    print(\"Skipping VLM — set GROK_API_KEY in Cell 2 to enable.\")\n",
    "    print(\"The pipeline still produces structured spatial data without VLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Visualizations\n",
    "# ============================================================\n",
    "from utils.visualize import (\n",
    "    plot_annotated_frames, plot_3d_scene, plot_trajectory_topdown,\n",
    "    plot_activity_timeline, plot_object_frequency, export_results\n",
    ")\n",
    "\n",
    "# Build depth maps list for visualization\n",
    "depth_maps_list = []\n",
    "for i in range(len(keyframes)):\n",
    "    fname = f\"{i:06d}.jpg\"\n",
    "    depth_maps_list.append(recon_data[\"depth_map_cache\"].get(fname))\n",
    "\n",
    "# Annotated frames + depth\n",
    "plot_annotated_frames(keyframes, scene_graphs, depth_maps_list, timestamps, OUTPUT_DIR)\n",
    "\n",
    "# Top-down trajectory\n",
    "plot_trajectory_topdown(recon_data[\"cam_positions_smooth\"], OUTPUT_DIR)\n",
    "\n",
    "# Object frequency\n",
    "plot_object_frequency(scene_graphs, OUTPUT_DIR)\n",
    "\n",
    "# Activity timeline (if VLM was run)\n",
    "if analysis_json:\n",
    "    plot_activity_timeline(analysis_json, OUTPUT_DIR)\n",
    "\n",
    "# Show the saved plots inline\n",
    "for img_name in [\"annotated_frames.png\", \"trajectory_topdown.png\", \"object_frequency.png\"]:\n",
    "    path = os.path.join(OUTPUT_DIR, img_name)\n",
    "    if os.path.exists(path):\n",
    "        from IPython.display import display, Image as IPImage\n",
    "        display(IPImage(filename=path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Export Results\n",
    "# ============================================================\n",
    "summary = export_results(\n",
    "    scene_graphs, analysis_json, recon_data[\"cam_positions_smooth\"],\n",
    "    object_labels, timestamps, VIDEO_PATH, OUTPUT_DIR\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PIPELINE COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nResults saved to: {os.path.abspath(OUTPUT_DIR)}/\")\n",
    "print(f\"  scene_graphs.json    — structured spatial data for LLM\")\n",
    "print(f\"  vlm_analysis.json    — activity classification\")\n",
    "print(f\"  camera_trajectory.npy — worker path\")\n",
    "print(f\"  summary.json         — overview stats\")\n",
    "print(f\"  memory_store/        — FAISS queryable memory\")\n",
    "print(f\"  *.png, *.html        — visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Interactive Memory Queries (run anytime after Cell 8)\n",
    "# ============================================================\n",
    "# You can re-run this cell with different queries\n",
    "\n",
    "# Change these to explore:\n",
    "QUERY_LABEL = \"block\"        # search for any label\n",
    "DEPTH_MIN = 0.5              # meters\n",
    "DEPTH_MAX = 2.0\n",
    "PROX_A = \"person\"            # proximity: label A near label B\n",
    "PROX_B = \"block\"\n",
    "PROX_MAX_M = 2.0\n",
    "\n",
    "print(f\"--- Label query: '{QUERY_LABEL}' ---\")\n",
    "results = memory.query_label(QUERY_LABEL)\n",
    "print(f\"  Found in {len(results)} frames\")\n",
    "if results:\n",
    "    r = results[0]\n",
    "    print(f\"  Example: frame {r['frame_idx']} at {r['timestamp_str']}\")\n",
    "    for d in r['detections']:\n",
    "        if QUERY_LABEL.lower() in d['label'].lower():\n",
    "            print(f\"    {d['label']}: depth={d['depth_m']}m, pos={d['position_3d']}\")\n",
    "\n",
    "print(f\"\\n--- Depth range: {DEPTH_MIN}-{DEPTH_MAX}m ---\")\n",
    "results = memory.query_depth_range(DEPTH_MIN, DEPTH_MAX)\n",
    "print(f\"  {len(results)} frames with objects in range\")\n",
    "\n",
    "print(f\"\\n--- Proximity: '{PROX_A}' within {PROX_MAX_M}m of '{PROX_B}' ---\")\n",
    "results = memory.query_proximity(PROX_A, PROX_B, max_m=PROX_MAX_M)\n",
    "print(f\"  {len(results)} frames\")\n",
    "for r in results[:3]:\n",
    "    print(f\"    frame {r['frame_idx']} at {r['timestamp_str']} — dist={r.get('_dist', '?')}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Batch Processing — Run on Multiple Videos\n",
    "# ============================================================\n",
    "import glob\n",
    "\n",
    "VIDEO_DIR = \"videos/IronsiteHackathonData\"\n",
    "BATCH_OUTPUT = \"output_batch\"\n",
    "\n",
    "videos = sorted(glob.glob(os.path.join(VIDEO_DIR, \"*.mp4\")))\n",
    "print(f\"Found {len(videos)} videos:\")\n",
    "for v in videos:\n",
    "    print(f\"  {os.path.basename(v)}\")\n",
    "\n",
    "# Uncomment below to run the full pipeline on all videos:\n",
    "# for video_path in videos:\n",
    "#     name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "#     out = os.path.join(BATCH_OUTPUT, name)\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Processing: {name}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     !python pipeline.py --video \"{video_path}\" --output \"{out}\" --skip-vlm\n",
    "#     print(f\"Done: {name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}